{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to ACMSRE For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to ACMSRE"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"design/","text":"Design Descisions Preface: https://www.redhat.com/en/blog/how-does-red-hat-support-day-2-operations Day 0 Operations Planning - How do we do Ops Project: AOC We use a multiple github repository to allow us to deploy and operatoe RHACM. For application and configuration data, we define kustomization overlays to cover each distinct deployment targets. Targets make up different stage environments or different region environments. We use Ansible Automation Platform to trigger Ansible Playbooks to build the infrastructure that make up the management components. This includes the ansible playbook to deploy OCP as a private cluster on Azure. We also define ansible playbooks to import AKS clusters into ACM. We need to support multiple ACM deployments across multiple regions. To limit costs, We support a two stage deployment workflow: development and production. Developers are still able to deploy sandbox environments, using an internal RHACM infrastructure cluster, with has support for cluster pools. We develop in the cloud. For the AOC project we use gitops with Openshift Gitops to deploy RHACM workload and configuration data. The image below is a sample the current set of ArgoCD applications we have defined. Project: KCP This project leverages an existing RHACM infrastructure to deploy the public OCP clusters into which we will deploy the RHACM instance to support the project. The initial OCP clusters are detached from the infrastrucutre RHACM. The deployment of RHACM into OCP clusters will be managed by Openshift Pipelines. Openshift Pipeline will deploy Openshift Gitops, and similar to the AOC project, we will use Argocd to handle the rollout of ACM and ACM configuration. Commonailty across all RHACM deployments Planning - Sizing One ACM Hub / Bastion Host per Region Currently, we support AKS cluster deployed across the NA region. A single ACM hub cluster will be available to import and manage all the AKS clusters in the North America region. The EMEA region will consist of Europe, Middle East, and Africa countries. Day 1 Operations New Environment Deployments The ansible project https://github.com/rcarrata/ocp4-azure-ipi was forked to https://github.com/stolostron/ocp4-azure-ipi and is used to deploy OCP clusters. Red Hat Ansibile Automation Platform is used to perform the Day 1 deployment of the Openshift Clusters. An ansible playbook from REPO is used to bootstrap the Openshift Gitops. Openshift Gitops handles rolling out and maintain the set of application and configuration to the ACM hub cluster, or local cluster. ACM policy is used for maintain configuration across the fleet of managed clusters. Project: AOC graph LR AAP --> |OCP IPI| OCP AAP --> |Openshift GitOps| OCP[OCP & RHACM] OCP --> |ArgoCD - Application & Configuration| OCP OCP --> |Policy| FLEET Project: KCP graph LR RHACM[RHACM*] --> |OCP IPI| OCP[OCP & RHACM] PIPELINE --> |Openshift GitOps| OCP OCP --> |ArgoCD - Application & Configuration| OCP OCP --> |Policy| FLEET Day 2 Operations Importing AKS Clusters AKS clusters are imported into the ACM hub as a Day 2 operation using an ansible playbook. The playbook is idempotent and runs against an inventory of AKS clusters. The inventory is dyanmic, where targets can be added and removed. In this way, a single playbook can be used against inventories from different stages--dev and production, or different regions. The following tasks are performed on each cluster: Setup networking to allow the AKS cluster to access the ACM k8s endpoint. Setup networking to allow the AAP cluster to access the AKS k8s endpoint. Generate the ManageCluster CR on the ACM hub. Read and apply the generated import secret on the AKS cluster. Reference See details at 8.2. Importing a managed cluster with the CLI As new AKS clusters come online, the ansible playbook running on a schedule will iterate over the inventory of clusters and import the clusters into ACM. Note If it takes 5m to run the import playbook from start to end for a single AKS cluster, then iterating over 100 AKS clusters, will take 500m or 8.3 hours, as a rough estimate. Enabling parallism will reduce the total time. Currently, the import procedure follows the out of the box import procedure, using a Service Account with cluster-admin rolebinding to generate the managed cluster namespace, and create the Managed Cluster CR. The best practice is always to use the mininal privilege as possible, so we question, if customers in general are ok with using a cluster-admin role to import all their clusters. An alternative import procedure is available, that uses a generic import payload to be applied on the AKS cluster, and a service account with minimal privledge. This requires the ACM hub to be configured to generate a generic payload. Using this procedure will simplify the import procedure by not having to access the ACM hub cluster during the import procedure. We would just need to access the target cluster. This alternative process will still be a Day 2 operation. Backup and Restore | Disaster Recovery TBD Upgrade - OCP We are following the following OCP upgrade policy: The ACM SRE maintenance window is between Sunday night (CST) and Wednesday morning (CST). OCP minor version release upgrades will happen during this window. We will deploy the latest version, weekly. OCP major version release upgrades will happen during this window. We will upgrade to the next release, 1 week after it is available. Development stage is upgraded first. Production(s) stage is upgraded next. Upgrade - ACM The upgrade of RHACM will happen during the upgrade maintenance window from Sunday night (CST) to Wedneday morning (CST). This allows us to provide business hour support across the maintenance windown. We will upgrade ACM on the maintenance window following the release of RHACM. Development stage is upgraded first. Production(s) stage is upgraded next. Gitops procedure for upgrading RHACM 2.4 to RHACM 2.5 There is a known blocker issue for upgrading RHACM from 2.4 to 2.5 when the cluster proxy addon is enabled. Before upgrading from 2.4 to 2.5 we also need to disable the backup feature. The following steps includes the workaround to allow the upgrade with Gitops. Create PR update MultiClusterHub CR to disable proxy addon spec : enableClusterProxyAddon : false Create PR to update ACM subscription spec : channel : release-2.5 installPlanApproval : Manual name : advanced-cluster-management source : redhat-operators sourceNamespace : openshift-marketplace Login to the OCP console to review and approve the install plan for release 2.5 upgrade. After the RHACM 2.5 upgrade completes successfully, create PR to update MultiClusterHub CR to re-enable the cluster proxy addon. apiVersion : operator.open-cluster-management.io/v1 kind : MultiClusterHub metadata : name : multiclusterhub namespace : open-cluster-management spec : overrides : components : - enabled : true name : cluster-proxy-addon - enabled : true name : multiclusterhub-repo - enabled : true name : search - enabled : true name : management-ingress - enabled : true name : console - enabled : true name : insights - enabled : true name : grc - enabled : true name : cluster-lifecycle - enabled : true name : volsync - enabled : true name : multicluster-engine - enabled : false name : cluster-backup Manually delete the secret cluster-proxy-signer and let cluster-proxy-addon-manager to refresh it. oc delete secret cluster-proxy-signer -n open-cluster-management Alerts and Alert Management TBD Observability TBD","title":"Design Descisions"},{"location":"design/#day-0-operations","text":"","title":"Day 0 Operations"},{"location":"design/#planning-how-do-we-do-ops","text":"","title":"Planning - How do we do Ops"},{"location":"design/#project-aoc","text":"We use a multiple github repository to allow us to deploy and operatoe RHACM. For application and configuration data, we define kustomization overlays to cover each distinct deployment targets. Targets make up different stage environments or different region environments. We use Ansible Automation Platform to trigger Ansible Playbooks to build the infrastructure that make up the management components. This includes the ansible playbook to deploy OCP as a private cluster on Azure. We also define ansible playbooks to import AKS clusters into ACM. We need to support multiple ACM deployments across multiple regions. To limit costs, We support a two stage deployment workflow: development and production. Developers are still able to deploy sandbox environments, using an internal RHACM infrastructure cluster, with has support for cluster pools. We develop in the cloud. For the AOC project we use gitops with Openshift Gitops to deploy RHACM workload and configuration data. The image below is a sample the current set of ArgoCD applications we have defined.","title":"Project: AOC"},{"location":"design/#project-kcp","text":"This project leverages an existing RHACM infrastructure to deploy the public OCP clusters into which we will deploy the RHACM instance to support the project. The initial OCP clusters are detached from the infrastrucutre RHACM. The deployment of RHACM into OCP clusters will be managed by Openshift Pipelines. Openshift Pipeline will deploy Openshift Gitops, and similar to the AOC project, we will use Argocd to handle the rollout of ACM and ACM configuration.","title":"Project: KCP"},{"location":"design/#commonailty-across-all-rhacm-deployments","text":"","title":"Commonailty across all RHACM deployments"},{"location":"design/#planning-sizing","text":"","title":"Planning - Sizing"},{"location":"design/#one-acm-hub-bastion-host-per-region","text":"Currently, we support AKS cluster deployed across the NA region. A single ACM hub cluster will be available to import and manage all the AKS clusters in the North America region. The EMEA region will consist of Europe, Middle East, and Africa countries.","title":"One ACM Hub / Bastion Host per Region"},{"location":"design/#day-1-operations","text":"","title":"Day 1 Operations"},{"location":"design/#new-environment-deployments","text":"The ansible project https://github.com/rcarrata/ocp4-azure-ipi was forked to https://github.com/stolostron/ocp4-azure-ipi and is used to deploy OCP clusters. Red Hat Ansibile Automation Platform is used to perform the Day 1 deployment of the Openshift Clusters. An ansible playbook from REPO is used to bootstrap the Openshift Gitops. Openshift Gitops handles rolling out and maintain the set of application and configuration to the ACM hub cluster, or local cluster. ACM policy is used for maintain configuration across the fleet of managed clusters. Project: AOC graph LR AAP --> |OCP IPI| OCP AAP --> |Openshift GitOps| OCP[OCP & RHACM] OCP --> |ArgoCD - Application & Configuration| OCP OCP --> |Policy| FLEET Project: KCP graph LR RHACM[RHACM*] --> |OCP IPI| OCP[OCP & RHACM] PIPELINE --> |Openshift GitOps| OCP OCP --> |ArgoCD - Application & Configuration| OCP OCP --> |Policy| FLEET","title":"New Environment Deployments"},{"location":"design/#day-2-operations","text":"","title":"Day 2 Operations"},{"location":"design/#importing-aks-clusters","text":"AKS clusters are imported into the ACM hub as a Day 2 operation using an ansible playbook. The playbook is idempotent and runs against an inventory of AKS clusters. The inventory is dyanmic, where targets can be added and removed. In this way, a single playbook can be used against inventories from different stages--dev and production, or different regions. The following tasks are performed on each cluster: Setup networking to allow the AKS cluster to access the ACM k8s endpoint. Setup networking to allow the AAP cluster to access the AKS k8s endpoint. Generate the ManageCluster CR on the ACM hub. Read and apply the generated import secret on the AKS cluster. Reference See details at 8.2. Importing a managed cluster with the CLI As new AKS clusters come online, the ansible playbook running on a schedule will iterate over the inventory of clusters and import the clusters into ACM. Note If it takes 5m to run the import playbook from start to end for a single AKS cluster, then iterating over 100 AKS clusters, will take 500m or 8.3 hours, as a rough estimate. Enabling parallism will reduce the total time. Currently, the import procedure follows the out of the box import procedure, using a Service Account with cluster-admin rolebinding to generate the managed cluster namespace, and create the Managed Cluster CR. The best practice is always to use the mininal privilege as possible, so we question, if customers in general are ok with using a cluster-admin role to import all their clusters. An alternative import procedure is available, that uses a generic import payload to be applied on the AKS cluster, and a service account with minimal privledge. This requires the ACM hub to be configured to generate a generic payload. Using this procedure will simplify the import procedure by not having to access the ACM hub cluster during the import procedure. We would just need to access the target cluster. This alternative process will still be a Day 2 operation.","title":"Importing AKS Clusters"},{"location":"design/#backup-and-restore-disaster-recovery","text":"TBD","title":"Backup and Restore | Disaster Recovery"},{"location":"design/#upgrade-ocp","text":"We are following the following OCP upgrade policy: The ACM SRE maintenance window is between Sunday night (CST) and Wednesday morning (CST). OCP minor version release upgrades will happen during this window. We will deploy the latest version, weekly. OCP major version release upgrades will happen during this window. We will upgrade to the next release, 1 week after it is available. Development stage is upgraded first. Production(s) stage is upgraded next.","title":"Upgrade - OCP"},{"location":"design/#upgrade-acm","text":"The upgrade of RHACM will happen during the upgrade maintenance window from Sunday night (CST) to Wedneday morning (CST). This allows us to provide business hour support across the maintenance windown. We will upgrade ACM on the maintenance window following the release of RHACM. Development stage is upgraded first. Production(s) stage is upgraded next.","title":"Upgrade - ACM"},{"location":"design/#gitops-procedure-for-upgrading-rhacm-24-to-rhacm-25","text":"There is a known blocker issue for upgrading RHACM from 2.4 to 2.5 when the cluster proxy addon is enabled. Before upgrading from 2.4 to 2.5 we also need to disable the backup feature. The following steps includes the workaround to allow the upgrade with Gitops. Create PR update MultiClusterHub CR to disable proxy addon spec : enableClusterProxyAddon : false Create PR to update ACM subscription spec : channel : release-2.5 installPlanApproval : Manual name : advanced-cluster-management source : redhat-operators sourceNamespace : openshift-marketplace Login to the OCP console to review and approve the install plan for release 2.5 upgrade. After the RHACM 2.5 upgrade completes successfully, create PR to update MultiClusterHub CR to re-enable the cluster proxy addon. apiVersion : operator.open-cluster-management.io/v1 kind : MultiClusterHub metadata : name : multiclusterhub namespace : open-cluster-management spec : overrides : components : - enabled : true name : cluster-proxy-addon - enabled : true name : multiclusterhub-repo - enabled : true name : search - enabled : true name : management-ingress - enabled : true name : console - enabled : true name : insights - enabled : true name : grc - enabled : true name : cluster-lifecycle - enabled : true name : volsync - enabled : true name : multicluster-engine - enabled : false name : cluster-backup Manually delete the secret cluster-proxy-signer and let cluster-proxy-addon-manager to refresh it. oc delete secret cluster-proxy-signer -n open-cluster-management","title":"Gitops procedure for upgrading RHACM 2.4 to RHACM 2.5"},{"location":"design/#alerts-and-alert-management","text":"TBD","title":"Alerts and Alert Management"},{"location":"design/#observability","text":"TBD","title":"Observability"},{"location":"fleet-health/","text":"Fleet Health Determining ACM Health The Red Hat Advanced Cluster Management product is delivered as an Operator Hub based operator. The Operator is deployed with a Subscription defintion. The instance Operand manifest is applied, requiring only a reference to an image pull secret. In the 2.4 release, we can verify the state of the ACM deployment by monitoring the status output of the multiclusterhub CR. The multiclusterhub CR reflects the state of the MulticlusterHub Operator , or MCH . If all the ACM components are successfully deployed, then the final status of the multiclusterhub CR will be Running . A state of Progressing indicates that the install process has not yet completed. A Blocked status indicates that the install or upgrade process was interrupted due to missing requirement or conflict that prevents the process from starting. In the 2.5 release, the cluster-lifecycle subcomponent was separated out as a standalone product component called Multicluster Engine , with a short name short name of MCE . When we deploy RHACM 2.5, both product component operators will be deployed. The Multiclusterhub operator and the Multicluster Engine operator. The MCH will refect the same status as before, with Running indicating sucessful completion of install or upgrade. The MCE will show a status of Available to indicate that its install/upgrade processing has completed successfully. graph LR MCH[MCH] -->|Subscribe| APPSUB_A(APPSUB_A) MCH[MCH] -->|Subscribe| APPSUB_B(APPSUB_B) MCH[MCH] -->|Subscribe| APPSUB_C(APPSUB_C) MCH[MCH] -->|Subscribe| APPSUB_D(APPSUB_D) MCH[MCH] -->|Subscribe| APPSUB_E(APPSUB_E) APPSUB_A[APPSUB_A] --> HELM_A[HELM_A] APPSUB_B[APPSUB_B] --> HELM_B[HELM_B] APPSUB_C[APPSUB_C] --> HELM_C[HELM_C] APPSUB_D[APPSUB_D] --> HELM_D[HELM_D] APPSUB_E[APPSUB_E] --> HELM_E[HELM_E] MCE[MCE] %% B --> C{Let me think} %% C -->|One| D[Laptop] %% C -->|Two| E[iPhone] %% C -->|Three| F[fa:fa-car Car] The MCH and MCE reflect the deploy time status of RHACM. We can derive the instantious pod state weither they are running or crashing, and historically if the pods have a history of restarting. From this we cannot tell if the internal function are nominal. Because we use Openshift Gitops to deploy RHACM as an ArgoCD application, we also inspect ArgoCD to determine the immediate state of the ACM component deployment (See Image 1 above). To determine ACM Heath from metrics and utilization, the Openshift Monitoriing components can be inspected. If ACM Observability is enabled, then the ACM Hub (local-cluster) is aggregated with the rest of the fleet data and available in the aggregate Grafana dashboard. What is needed We need to understand the API latency on each ACM subcomponent. We need to be able to measure ACM to determine SLO. The fact that ACM is a workload running on top of Openshift, knowing the k8s apiserver SLO, does not necessarily reflect the SLO for the ACM component api. There is no specific dashboard that shows data for the ACM components. Up to now, we manually create the view, without ever persisting the views to a reusable custom dashboard. Determine Fleet Health In order to review the fleet health or status, we can inspect ACM and Observability Grafana dashboard. The ACM cluster view indicates the current state of imported clusters, if they are Ready , or not Ready . The Goverance and Application views indicates the state of policy compliance and applications respectiveliy, across the fleet. Finally, the Observability dashboard aggregates metrics data from the fleet in a single pane of glass. Note There is one important notion that our work has highlighted. ACM as a system does not represent the source of truth clusters in the enterprise. We can import clusters from different cloud providers and even on-prem sources, but those providers will be the source of truth of their domain. ACM can span providers, but still, does not represent a source of truth. ACM is only aware of what is given it. Clusters can disappear. ACM does have the capability to create new clusters via the hive component or assisted installer. What is needed The SRE will need to define custom dashboards to display views specific to their needs. Workload metrics data is collected, but the out of the box Openshift dashboards is a general view. You workload components may not export useful data, so you will likely go through a cycle of improvement, and re-render dashboard view. Since we started with AOC/AAP, we have not produced any new custom dashboards, beyond the AAP golden signals. The process to create a custom dashboard, and exported out to source repo, and imported back in is captured as a script. But wouldn't this be better if its rolled up in the Openshift cluster as an Openshift Pipeline (tekton).","title":"Fleet Health"},{"location":"fleet-health/#determining-acm-health","text":"The Red Hat Advanced Cluster Management product is delivered as an Operator Hub based operator. The Operator is deployed with a Subscription defintion. The instance Operand manifest is applied, requiring only a reference to an image pull secret. In the 2.4 release, we can verify the state of the ACM deployment by monitoring the status output of the multiclusterhub CR. The multiclusterhub CR reflects the state of the MulticlusterHub Operator , or MCH . If all the ACM components are successfully deployed, then the final status of the multiclusterhub CR will be Running . A state of Progressing indicates that the install process has not yet completed. A Blocked status indicates that the install or upgrade process was interrupted due to missing requirement or conflict that prevents the process from starting. In the 2.5 release, the cluster-lifecycle subcomponent was separated out as a standalone product component called Multicluster Engine , with a short name short name of MCE . When we deploy RHACM 2.5, both product component operators will be deployed. The Multiclusterhub operator and the Multicluster Engine operator. The MCH will refect the same status as before, with Running indicating sucessful completion of install or upgrade. The MCE will show a status of Available to indicate that its install/upgrade processing has completed successfully. graph LR MCH[MCH] -->|Subscribe| APPSUB_A(APPSUB_A) MCH[MCH] -->|Subscribe| APPSUB_B(APPSUB_B) MCH[MCH] -->|Subscribe| APPSUB_C(APPSUB_C) MCH[MCH] -->|Subscribe| APPSUB_D(APPSUB_D) MCH[MCH] -->|Subscribe| APPSUB_E(APPSUB_E) APPSUB_A[APPSUB_A] --> HELM_A[HELM_A] APPSUB_B[APPSUB_B] --> HELM_B[HELM_B] APPSUB_C[APPSUB_C] --> HELM_C[HELM_C] APPSUB_D[APPSUB_D] --> HELM_D[HELM_D] APPSUB_E[APPSUB_E] --> HELM_E[HELM_E] MCE[MCE] %% B --> C{Let me think} %% C -->|One| D[Laptop] %% C -->|Two| E[iPhone] %% C -->|Three| F[fa:fa-car Car] The MCH and MCE reflect the deploy time status of RHACM. We can derive the instantious pod state weither they are running or crashing, and historically if the pods have a history of restarting. From this we cannot tell if the internal function are nominal. Because we use Openshift Gitops to deploy RHACM as an ArgoCD application, we also inspect ArgoCD to determine the immediate state of the ACM component deployment (See Image 1 above). To determine ACM Heath from metrics and utilization, the Openshift Monitoriing components can be inspected. If ACM Observability is enabled, then the ACM Hub (local-cluster) is aggregated with the rest of the fleet data and available in the aggregate Grafana dashboard.","title":"Determining ACM Health"},{"location":"fleet-health/#what-is-needed","text":"We need to understand the API latency on each ACM subcomponent. We need to be able to measure ACM to determine SLO. The fact that ACM is a workload running on top of Openshift, knowing the k8s apiserver SLO, does not necessarily reflect the SLO for the ACM component api. There is no specific dashboard that shows data for the ACM components. Up to now, we manually create the view, without ever persisting the views to a reusable custom dashboard.","title":"What is needed"},{"location":"fleet-health/#determine-fleet-health","text":"In order to review the fleet health or status, we can inspect ACM and Observability Grafana dashboard. The ACM cluster view indicates the current state of imported clusters, if they are Ready , or not Ready . The Goverance and Application views indicates the state of policy compliance and applications respectiveliy, across the fleet. Finally, the Observability dashboard aggregates metrics data from the fleet in a single pane of glass. Note There is one important notion that our work has highlighted. ACM as a system does not represent the source of truth clusters in the enterprise. We can import clusters from different cloud providers and even on-prem sources, but those providers will be the source of truth of their domain. ACM can span providers, but still, does not represent a source of truth. ACM is only aware of what is given it. Clusters can disappear. ACM does have the capability to create new clusters via the hive component or assisted installer.","title":"Determine Fleet Health"},{"location":"fleet-health/#what-is-needed_1","text":"The SRE will need to define custom dashboards to display views specific to their needs. Workload metrics data is collected, but the out of the box Openshift dashboards is a general view. You workload components may not export useful data, so you will likely go through a cycle of improvement, and re-render dashboard view. Since we started with AOC/AAP, we have not produced any new custom dashboards, beyond the AAP golden signals. The process to create a custom dashboard, and exported out to source repo, and imported back in is captured as a script. But wouldn't this be better if its rolled up in the Openshift cluster as an Openshift Pipeline (tekton).","title":"What is needed"}]}